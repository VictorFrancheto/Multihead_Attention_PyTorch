{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b940e30b",
   "metadata": {},
   "source": [
    "# Implementation of Layer Head Attention in PyTorch\n",
    "\n",
    "#### How implementation of layer head attention in PyTorch?\n",
    "\n",
    "\n",
    "Before we proceed with the step-by-step implementation of the attention layer, it's essential to highlight that prior knowledge of PyTorch, particularly `torch.nn` and `torch.nn.functional`, along with familiarity with the Transformer architecture, is required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafba2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Imports the main PyTorch library for tensor manipulation and related operations.\n",
    "import torch.nn as nn  # Imports the `torch.nn` module, which provides classes for building neural networks.\n",
    "import torch.nn.functional as F  # Imports auxiliary functions from PyTorch used in neural network operations.\n",
    "\n",
    "# Checks if a GPU is available; if not, it defaults to using the CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prints to the console which device will be used (CPU or GPU).\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324085d3",
   "metadata": {},
   "source": [
    "The `torch.device` configuration is crucial for training models on a GPU, particularly for large-scale models where the attention mechanism demands significant computational resources.\n",
    "\n",
    "Every attention mechanism is built upon three fundamental components: the **Query**, **Key**, and **Value** matrices.\n",
    "\n",
    "These matrices form the foundation of the attention layer, enabling the model to efficiently focus on specific parts of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        # Defining linear transformations for Query, Key, and Value matrices\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Computing the Query, Key, and Value matrices\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        \n",
    "        return Q, K, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7544a89",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "### What We’re Doing:\n",
    "\n",
    "- The **Query**, **Key**, and **Value** matrices are initialized as `nn.Linear` layers, all sharing the same embedding dimension (`embed_dim`). This uniform dimensionality simplifies the computation of attention scores through matrix multiplication in subsequent steps.\n",
    "- In the `forward` method, the input tensor `x` is passed through these linear layers to generate the **Query**, **Key**, and **Value** matrices.\n",
    "- The main advantage of this approach lies in learning: the linear transformations allow the model to learn optimal weights during training, dynamically adjusting how attention focuses on different parts of the input.\n",
    "\n",
    "Additionally, by leveraging PyTorch’s `nn.Linear` layers, we streamline the implementation, avoiding manual weight handling and bias calculations, resulting in cleaner and more efficient code.\n",
    "\n",
    "## Scaled Dot-Product Attention Implementation\n",
    "\n",
    "The **Scaled Dot-Product Attention** calculates a set of scores that determine the \"focus\" of attention for each position in the input.\n",
    "\n",
    "The scaling of these scores plays a critical role: it prevents excessively large values that could lead to unstable gradients during backpropagation, ensuring a more stable and efficient training process.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "1. **Calculate Attention Scores:** We use `torch.matmul` to compute the dot product between the **Query** and **Key** matrices, producing the raw attention scores.  \n",
    "2. **Scale by Key Dimension:** The scores are divided by the square root of the **Key** dimension to ensure gradient stability during training.  \n",
    "3. **Normalize with Softmax:** The Softmax function converts the scores into probabilities, allowing each position in the sequence to \"focus\" on others proportionally.  \n",
    "\n",
    "---\n",
    "\n",
    "## Code Snippet\n",
    "\n",
    "Next, we implement **Scaled Dot-Product Attention** as part of the `AttentionLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale_factor = embed_dim ** 0.5  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        \n",
    "        # Compute the scaled dot-product attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale_factor\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Output weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c080",
   "metadata": {},
   "source": [
    "## Essential Optimizations and Potential Challenges\n",
    "\n",
    "\n",
    "### Numerical Stability\n",
    "When your model works with high-dimensional embeddings, attention scores can become excessively large, leading to numerical instability and disrupting the training process.\n",
    "\n",
    "#### Recommended Solution:\n",
    "Use `torch.clamp` to restrict the values within a safe range. This prevents value explosions and unstable gradients, ensuring more stable and efficient training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30815c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attention scores using torch.clamp for numerical stability\n",
    "scores = torch.matmul(query, key.transpose(-2, -1)) / (key.size(-1) ** 0.5)\n",
    "scores = torch.clamp(scores, min=-1e9, max=1e9)\n",
    "\n",
    "# Apply Softmax to the adjusted scores\n",
    "attention_weights = F.softmax(scores, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d3e1f",
   "metadata": {},
   "source": [
    "Note that, the use of `torch.matmul` enables the efficient calculation of attention scores and weighted outputs in a single operation. This avoids the need for loops, reduces computational complexity, and optimizes memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adf011",
   "metadata": {},
   "source": [
    "## Building the Multi-Head Attention Layer\n",
    "\n",
    "**Multi-Head Attention** (MHA) takes the concept of attention further by allowing multiple independent \"heads\" to process different aspects of the input data simultaneously.\n",
    "\n",
    "The implementation involves splitting the input into multiple heads, computing attention for each head individually, and finally concatenating them to produce the combined output.\n",
    "\n",
    "### Splitting Inputs for Multi-Head\n",
    "\n",
    "It might seem complex, but splitting into multiple heads is easier than you think.\n",
    "\n",
    "The **query**, **key**, and **value** matrices are divided into separate heads, reshaped to `[batch_size, num_heads, seq_length, head_dim]`. After processing, they are concatenated back together to reconstruct the final output.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "Below is a reusable implementation of MHA, leveraging `view` and `permute` for efficient reshaping of matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Define linear layers for Q, K, V transformations\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # Reshape and split into heads\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)  # Rearrange to [batch, num_heads, seq_length, head_dim]\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Concatenate heads back together\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # Rearrange to [batch, seq_length, num_heads, head_dim]\n",
    "        return x.view(batch_size, seq_length, num_heads * head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.split_heads(self.query_layer(x))\n",
    "        K = self.split_heads(self.key_layer(x))\n",
    "        V = self.split_heads(self.value_layer(x))\n",
    "        \n",
    "        # Scaled Dot-Product Attention for each head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        multihead_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Combine heads and apply output projection\n",
    "        multihead_output = self.combine_heads(multihead_output)\n",
    "        return self.out_proj(multihead_output), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5a54b",
   "metadata": {},
   "source": [
    "The use of `view` and `permute` in this way minimizes reshaping operations, which can often become bottlenecks in PyTorch models. By avoiding excessive reshaping, we save memory and ensure that the MHA layer operates with maximum efficiency.\n",
    "\n",
    "\n",
    "## Integrating Attention Mechanisms into Models\n",
    "\n",
    "With your MHA layer ready, the next step is to make it reusable across different models.\n",
    "\n",
    "I’ll demonstrate how to integrate it in a simple and flexible way into various architectures, such as RNNs, CNNs, or Transformers, expanding its applicability to suit different requirements.\n",
    "\n",
    "## Class-Based Implementation\n",
    "\n",
    "We’ll create an `AttentionLayer` class that encapsulates the full attention logic. Additionally, we’ll include support for dropout and weight initialization, providing greater flexibility and adaptability for use in different training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50134227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.multihead_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Optional weight initialization for more stable training\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.query_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.key_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.value_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.out_proj.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, attn_weights = self.multihead_attn(x)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f823c",
   "metadata": {},
   "source": [
    "- Initialized the MHA with a specified dropout for regularization, which reduces the risk of overfitting by randomly deactivating a fraction of the neurons during training.\n",
    "- Applied `nn.init.xavier_uniform_` for weight initialization, ensuring smooth and stable gradient flow during training.\n",
    "\n",
    "## Adding Positional Encoding (Optional for Transformer-Based Models)\n",
    "\n",
    "Transformers lack an intrinsic sense of order. They treat inputs as a collection of tokens without considering their relative position. This means that, without positional encoding, a Transformer cannot distinguish between sequences like \"A follows B\" and \"B follows A.\"\n",
    "\n",
    "Positional encoding solves this issue by providing positional information for each token in the input. This mechanism is essential for sequence-based tasks, such as Natural Language Processing (NLP) and models dealing with time-series data.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation\n",
    "\n",
    "#### Why Do We Need Positional Encoding?\n",
    "The Transformer architecture relies solely on attention and matrix-based operations, without directly considering token positions in a sequence. To capture the sequential structure, positional encoding assigns a unique \"positional identity\" to each token.\n",
    "\n",
    "#### How Does Positional Encoding Work?\n",
    "Positional encoding is typically implemented using sine and cosine functions at different frequencies. These functions generate values that vary smoothly with the position of tokens, enabling the model to distinguish tokens based on their order in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of This Approach\n",
    "\n",
    "- **Smooth Gradients:** The continuous variation of sine and cosine functions helps maintain smooth gradients, facilitating better learning.  \n",
    "- **Computational Efficiency:** Positional encoding is directly added to the input embeddings without requiring additional layers or computationally expensive operations.  \n",
    "- **Generalization:** The periodic structure of the functions allows the model to generalize well to sequences of varying lengths, as long as they remain within the predefined range.\n",
    "\n",
    "## Implementing the Positional Encoding Class\n",
    "\n",
    "Below is an efficient implementation of positional encoding in PyTorch. The `PositionalEncoding` class generates encodings based on sine and cosine functions, which can be directly added to the input embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c96ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encodings matrix with size [max_len, embed_dim]\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        \n",
    "        # Apply sin to even indices in embedding dimension, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as a buffer to avoid tracking in gradients\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: [1, max_len, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154ccde",
   "metadata": {},
   "source": [
    "## Optimization \n",
    "\n",
    "If you're working with long sequences or aiming to save computational resources, consider **generating positional encodings dynamically** instead of precomputing them.\n",
    "\n",
    "Precomputing positional encodings for all tokens in long sequences (or multiple sequences) can lead to unnecessary memory consumption, especially in applications that handle large data batches or real-time tasks. By generating positional encodings **on-demand**, you only create what's needed for each input, reducing memory usage and improving model scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## Testing the Attention Layer\n",
    "\n",
    "Ensuring your attention layer works correctly is essential to avoid issues later. This is where **unit testing** becomes invaluable.\n",
    "\n",
    "When testing specific components, focus on:\n",
    "- **Attention Scores:** Validate that attention score calculations are accurate.\n",
    "- **Dimensions:** Confirm that input and output tensors have the expected shapes.\n",
    "- **Gradient Flow:** Ensure that gradients are propagating correctly through the layer.\n",
    "\n",
    "These tests can help identify issues quickly and prevent long debugging sessions, especially with custom implementations of attention mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## Unit Testing: Verifying Attention Outputs\n",
    "\n",
    "Unit tests for the attention layer include:\n",
    "\n",
    "1. **Output and Weight Dimensions:**  \n",
    "   Ensure the attention output and weights have the expected dimensions. For instance, if the input has dimensions `[batch_size, seq_length, embed_dim]`, the output and weights should align accordingly.\n",
    "\n",
    "2. **Sum of Weights:**  \n",
    "   After applying Softmax to the attention scores, verify that the sum of weights for each sequence equals 1. This is critical since Softmax normalizes the scores into probabilities.\n",
    "\n",
    "3. **Consistency Across Runs:**  \n",
    "   Run the layer with the same input multiple times and confirm that it produces consistent results, unless dropout is enabled.\n",
    "\n",
    "4. **Validating Values:**  \n",
    "   Use functions like `torch.allclose` to compare the layer's results with expected values within a tolerance margin, ensuring the implementation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAttentionLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.embed_dim = 64\n",
    "        self.num_heads = 8\n",
    "        self.seq_len = 10\n",
    "        self.attention_layer = AttentionLayer(embed_dim=self.embed_dim, num_heads=self.num_heads)\n",
    "        self.input_tensor = torch.randn(1, self.seq_len, self.embed_dim)\n",
    "\n",
    "    def test_attention_output_shape(self):\n",
    "        output, attn_weights = self.attention_layer(self.input_tensor)\n",
    "        self.assertEqual(output.shape, (1, self.seq_len, self.embed_dim))\n",
    "        self.assertEqual(attn_weights.shape, (1, self.num_heads, self.seq_len, self.seq_len))\n",
    "\n",
    "    def test_attention_weights_sum(self):\n",
    "        _, attn_weights = self.attention_layer(self.input_tensor)\n",
    "        self.assertTrue(torch.allclose(attn_weights.sum(dim=-1), torch.tensor(1.0), atol=1e-6))\n",
    "\n",
    "    def test_gradients_exist(self):\n",
    "        output, _ = self.attention_layer(self.input_tensor)\n",
    "        output.sum().backward()\n",
    "        for param in self.attention_layer.parameters():\n",
    "            self.assertIsNotNone(param.grad)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0724c1",
   "metadata": {},
   "source": [
    "### Example: Integrating the Attention Layer into a Model  \n",
    "In this example, we will demonstrate how to integrate an attention layer into a larger model by combining it with an LSTM layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_dim, num_heads):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = AttentionLayer(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Example: outputting a single value (e.g., for regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "        attn_out, attn_weights = self.attention(lstm_out)  # Apply attention on LSTM output\n",
    "        final_output = self.fc(attn_out[:, -1, :])  # Example: using the last time step\n",
    "        \n",
    "        return final_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ee7c4",
   "metadata": {},
   "source": [
    "- **LSTM Layer:** The LSTM processes sequential data, capturing temporal dependencies and generating a contextual representation for each time step.  \n",
    "- **Attention Layer:** The output of the LSTM is fed into the attention layer, which selectively focuses on relevant parts of the sequence, potentially improving the model's understanding of the most important features.  \n",
    "- **Final Output:** For simplicity, a fully connected layer is applied to the last time step, configuring the model for a regression or classification task.  \n",
    "\n",
    "### Forward Pass Example\n",
    "Below, we illustrate how to implement the data flow within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example forward pass\n",
    "model = LSTMWithAttention(input_dim=128, hidden_dim=64, embed_dim=64, num_heads=8)\n",
    "example_input = torch.randn(32, 10, 128)  # Batch of 32, sequence length of 10, input dim of 128\n",
    "output, attn_weights = model(example_input)\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [32, 1]\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected shape: [32, 8, 10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da4334",
   "metadata": {},
   "source": [
    "In this example, we integrated attention within an LSTM model, but this technique can easily be applied to various architectures, including CNNs or fully connected networks.\n",
    "\n",
    "With these steps completed, you now have a fully functional and tested attention mechanism, ready to be implemented in complex architectures.\n",
    "\n",
    "This setup, combined with thorough testing and integration, enhances your model with advanced interpretive power and dynamic feature focus, potentially improving performance in complex tasks.\n",
    "\n",
    "\n",
    "### Handling Large Tensors with Mixed-Precision Training\n",
    "\n",
    "A practical way to reduce memory consumption without compromising model quality is to use mixed-precision training.\n",
    "\n",
    "This approach leverages `torch.cuda.amp` (automatic mixed precision), which casts operations to half precision (`float16`) whenever possible while maintaining critical computations in full precision (`float32`).\n",
    "\n",
    "This strategy can significantly boost performance by allowing faster training while reducing memory usage.\n",
    "\n",
    "Here’s a practical setup for implementing mixed precision using PyTorch’s `torch.cuda.amp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaff65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Example model, optimizer, and scaler setup\n",
    "model = AttentionLayer(embed_dim=64, num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()  # For automatic gradient scaling\n",
    "\n",
    "# Mixed-precision training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():  # Enable mixed precision\n",
    "            output, _ = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "        \n",
    "        # Backpropagation with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee2eee",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **autocast():** Executes all operations within this block in mixed precision, reducing memory usage while maintaining computational accuracy.  \n",
    "- **GradScaler:** Adjusts gradients to prevent underflow in `float16`, which is particularly useful when training with sensitive or high-resolution data.  \n",
    "\n",
    "This approach not only reduces memory consumption but also accelerates training, making it ideal for large-scale, attention-based models.\n",
    "\n",
    "\n",
    "## Benchmarking Tips for Attention Computations\n",
    "\n",
    "To evaluate the performance of your attention layer, you can leverage `torch.utils.benchmark`, which allows you to measure computation times and identify potential bottlenecks.  \n",
    "\n",
    "Benchmarking is particularly valuable when testing different model configurations or assessing performance across various hardware setups.\n",
    "\n",
    "\n",
    "## Benchmarking Attention Computation\n",
    "\n",
    "By using benchmarking tools, you can analyze the time taken by your attention layer and optimize its efficiency for production-level performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80758214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "# Sample data for benchmarking\n",
    "input_tensor = torch.randn(32, 10, 64).to(device)\n",
    "\n",
    "# Create benchmark timer\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"model(input_tensor)\",\n",
    "    globals={\"model\": model, \"input_tensor\": input_tensor},\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "time_taken = timer.timeit(100)  # Run the forward pass 100 times\n",
    "print(f\"Average time per forward pass: {time_taken.mean * 1e3:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a35f4",
   "metadata": {},
   "source": [
    "- **benchmark.Timer:** Tracks the average time taken for a specific operation, such as executing the forward pass of the attention layer.  \n",
    "- **Result Interpretation:** Analyze the average execution time per forward pass to assess the efficiency of your model and compare it under various conditions (e.g., CPU versus GPU, mixed precision versus full precision).  \n",
    "\n",
    "Implementing these performance insights ensures that your attention layer is not only functional but also optimized for handling large-scale workloads and real-time scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading and Advanced Topics\n",
    "\n",
    "To expand your understanding of attention mechanisms, consider diving into the following:\n",
    "\n",
    "- **Optimized Transformer Libraries:** Explore libraries like Hugging Face’s `transformers` or PyTorch’s `torchtext`, which offer efficient tools for working with Transformer-based architectures.  \n",
    "- **Multimodal Attention:** Experiment with combining attention across diverse data types, such as text and images, to unlock significant performance gains in multimodal models.  \n",
    "\n",
    "These tools and concepts will empower you to create even more advanced and flexible models, expanding the possibilities of what attention mechanisms can achieve in modern machine learning applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
