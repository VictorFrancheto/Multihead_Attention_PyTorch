{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558c578c",
   "metadata": {},
   "source": [
    "#### Como Implementar uma Camada de Atenção no PyTorch?\n",
    "\n",
    "Pré-requisitos  \n",
    "\n",
    "Antes de começarmos, há algumas coisas que você precisará em sua bagagem:  \n",
    "\n",
    "- Conhecimento intermediário a avançado em PyTorch, especialmente sobre `torch.nn` e `torch.nn.functional`.  \n",
    "- Familiaridade com a arquitetura Transformer — compreender como as camadas de atenção se encaixam nela ajudará a adaptar este guia às suas necessidades.  \n",
    "- Alguma experiência com técnicas de otimização para GPU no PyTorch, já que abordaremos a compatibilidade com dispositivos para garantir um bom desempenho.  \n",
    "\n",
    "#### Preparando o Ambiente do PyTorch e Importações\n",
    "Vamos colocar a mão na massa. Aqui está uma configuração básica para iniciar a implementação da sua camada de atenção.\n",
    "\n",
    "Código: Importações Essenciais e Configuração\n",
    "Começaremos importando as bibliotecas principais e configurando a compatibilidade com o dispositivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafba2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuração do dispositivo (CPU ou GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d224a2",
   "metadata": {},
   "source": [
    "## Aqui está algo que vale a pena destacar\n",
    "\n",
    "A configuração do `torch.device` é essencial se você pretende treinar modelos em uma GPU. Isso é especialmente útil para modelos em larga escala, já que o mecanismo de atenção pode ser computacionalmente intenso.\n",
    "\n",
    "Pode ser tentador ignorar a compatibilidade com o dispositivo no início, mas eu encorajo configurá-la logo no começo para evitar refatorações desnecessárias mais tarde.\n",
    "\n",
    "---\n",
    "\n",
    "## Componentes-chave de uma Camada de Atenção\n",
    "\n",
    "Agora é hora de arregaçar as mangas. Todo mecanismo de atenção depende de três componentes principais: as matrizes de **Query**, **Key** e **Value**.\n",
    "\n",
    "Elas formam a espinha dorsal de qualquer camada de atenção, permitindo que o modelo foque em partes específicas do input.\n",
    "\n",
    "Você pode estar se perguntando: \"Por que três matrizes separadas?\" Aqui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        # Defining linear transformations for Query, Key, and Value matrices\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Computing the Query, Key, and Value matrices\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        \n",
    "        return Q, K, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44be6",
   "metadata": {},
   "source": [
    "## Explicação\n",
    "\n",
    "### O que estamos fazendo:\n",
    "\n",
    "- As camadas de **Query**, **Key** e **Value** são inicializadas como camadas `nn.Linear` com a mesma dimensão de embedding (`embed_dim`). Essa dimensionalidade permite que os scores de atenção sejam calculados facilmente através de multiplicação matricial nas etapas seguintes.\n",
    "- No método `forward`, passamos `x` (nosso tensor de entrada) por cada uma dessas transformações lineares para produzir as matrizes de **Query**, **Key** e **Value**.\n",
    "- Aqui está a principal vantagem de defini-las dessa maneira: as transformações lineares permitem que o modelo aprenda os pesos apropriados durante o treinamento, adaptando dinamicamente como a atenção se concentra em diferentes partes da entrada.\n",
    "\n",
    "Além disso, ao usar a `nn.Linear` embutida do PyTorch, simplificamos o código, evitando a necessidade de manipulação manual de pesos e cálculos de bias.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementação da Atenção com Produto Escalar Escalado\n",
    "\n",
    "Aqui é onde a mágica acontece. A **Atenção com Produto Escalar Escalado** calcula um conjunto de scores de atenção que determinam o \"foco\" para cada posição na entrada.\n",
    "\n",
    "Ao escalar esses scores, evitamos valores excessivamente grandes que poderiam levar a gradientes instáveis durante a retropropagação.\n",
    "\n",
    "---\n",
    "\n",
    "### Código Passo a Passo\n",
    "\n",
    "1. **Calcular os Scores de Atenção:** Usamos `torch.matmul` para calcular o produto escalar entre as matrizes de **Query** e **Key**, obtendo os scores de atenção brutos.\n",
    "2. **Escalar pelo Tamanho da Dimensão:** Para manter os gradientes estáveis, dividimos pela raiz quadrada da dimensão de **Key**.\n",
    "3. **Aplicar Softmax para Normalizar os Scores:** O Softmax converte os scores em probabilidades, permitindo que cada posição \"foque\" em outras posições de forma relativa.\n",
    "\n",
    "\n",
    "## Trecho de Código\n",
    "\n",
    "Vamos implementar a **Atenção com Produto Escalar Escalado** como parte da `AttentionLayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale_factor = embed_dim ** 0.5  # Square root of embed dimension for scaling\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        \n",
    "        # Compute the scaled dot-product attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale_factor\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Output weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c080",
   "metadata": {},
   "source": [
    "## Otimizações Principais e Problemas Potenciais\n",
    "\n",
    "### Estabilidade Numérica\n",
    "Se o seu modelo trabalha com embeddings de alta dimensionalidade, os scores de atenção podem se tornar excessivamente grandes, levando a problemas de instabilidade numérica.  \n",
    "\n",
    "#### Solução Comum:\n",
    "Utilize `torch.clamp` para manter os valores dentro de um intervalo seguro. Isso ajuda a evitar explosões nos valores e gradientes instáveis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30815c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular os scores de atenção com torch.clamp para estabilidade numérica\n",
    "scores = torch.matmul(query, key.transpose(-2, -1)) / (key.size(-1) ** 0.5)\n",
    "scores = torch.clamp(scores, min=-1e9, max=1e9)\n",
    "\n",
    "# Aplicar Softmax nos scores ajustados\n",
    "attention_weights = F.softmax(scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.clamp(scores, min=-torch.finfo(scores.dtype).max, max=torch.finfo(scores.dtype).max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d3e1f",
   "metadata": {},
   "source": [
    "2. Multiplicações Matriciais Eficientes: O uso de `torch.matmul` permite calcular os scores de atenção e os outputs ponderados de forma eficiente em uma única execução, evitando a necessidade de loops e mantendo o uso de memória sob controle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adf011",
   "metadata": {},
   "source": [
    "## Construindo a Camada de Atenção Multi-Head\n",
    "\n",
    "A **Atenção Multi-Head** (MHA) leva o conceito de atenção um passo adiante, permitindo que múltiplas \"cabeças\" de atenção independentes aprendam diferentes aspectos dos dados de entrada.\n",
    "\n",
    "A implementação envolve dividir a entrada entre as cabeças, calcular a atenção para cada uma e, em seguida, concatená-las novamente.\n",
    "\n",
    "---\n",
    "\n",
    "### Configurando as Divisões Multi-Head\n",
    "\n",
    "Isso pode te surpreender: dividir em múltiplas cabeças é mais simples do que parece.\n",
    "\n",
    "Dividimos as matrizes de **query**, **key** e **value** em cabeças separadas, ajustando-as para o formato `[batch_size, num_heads, seq_length, head_dim]`.\n",
    "\n",
    "Depois, após o processamento, as concatenamos novamente.\n",
    "\n",
    "---\n",
    "\n",
    "## Trecho de Código\n",
    "\n",
    "Aqui está uma implementação reutilizável de MHA com um reshaping eficiente das matrizes usando `view` e `permute`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Define linear layers for Q, K, V transformations\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # Reshape and split into heads\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)  # Rearrange to [batch, num_heads, seq_length, head_dim]\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Concatenate heads back together\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # Rearrange to [batch, seq_length, num_heads, head_dim]\n",
    "        return x.view(batch_size, seq_length, num_heads * head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.split_heads(self.query_layer(x))\n",
    "        K = self.split_heads(self.key_layer(x))\n",
    "        V = self.split_heads(self.value_layer(x))\n",
    "        \n",
    "        # Scaled Dot-Product Attention for each head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        multihead_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Combine heads and apply output projection\n",
    "        multihead_output = self.combine_heads(multihead_output)\n",
    "        return self.out_proj(multihead_output), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5a54b",
   "metadata": {},
   "source": [
    "## Considerações de Eficiência\n",
    "\n",
    "O uso de `view` e `permute` dessa maneira minimiza as operações de reformatação, que podem se tornar um gargalo em modelos PyTorch. Evitar reformatações excessivas economiza memória e garante que sua camada de MHA funcione da forma mais eficiente possível.\n",
    "\n",
    "---\n",
    "\n",
    "## Integrando Mecanismos de Atenção em Modelos\n",
    "\n",
    "Agora que você tem sua MHA, vamos tornar esse módulo de atenção reutilizável em diferentes modelos.\n",
    "\n",
    "Vou mostrar como integrá-lo de forma simples e versátil para diferentes arquiteturas, como RNNs, CNNs ou Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementação Baseada em Classe\n",
    "\n",
    "Vamos criar uma classe `AttentionLayer`, encapsulando toda a lógica de atenção. Nessa classe, também adicionaremos opções para dropout e inicialização de pesos, proporcionando máxima flexibilidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50134227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.multihead_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Optional weight initialization for more stable training\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.query_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.key_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.value_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.multihead_attn.out_proj.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, attn_weights = self.multihead_attn(x)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4269973",
   "metadata": {},
   "source": [
    "## Aqui, nós:\n",
    "\n",
    "- Inicializamos a MHA com um dropout especificado para regularização.\n",
    "- Aplicamos `nn.init.xavier_uniform_` para a inicialização dos pesos, garantindo um fluxo de gradiente suave durante o treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "## Adicionando Codificação Posicional (Opcional para Modelos Baseados em Transformer)\n",
    "\n",
    "Isso pode te surpreender: os Transformers não possuem um senso intrínseco de ordem. Sem codificação posicional, eles tratariam os inputs como se estivessem desordenados.\n",
    "\n",
    "A codificação posicional dá a cada token um “lugar,” algo essencial para dados baseados em sequência, onde a ordem é importante, como em tarefas de NLP.\n",
    "\n",
    "---\n",
    "\n",
    "### Explicação Rápida\n",
    "\n",
    "Em modelos de sequência, a codificação posicional atribui a cada token uma embedding única e consciente de posição, permitindo que o Transformer distinga tokens com base em sua posição dentro da sequência.\n",
    "\n",
    "A maioria das implementações utiliza funções seno e cosseno em diferentes frequências para essas codificações, oferecendo um gradiente suave ao longo do tempo.\n",
    "\n",
    "Essa codificação posicional é então adicionada às embeddings de entrada, fornecendo ao modelo informações específicas de posição sem a necessidade de camadas extras ou cálculos complexos.\n",
    "\n",
    "## Código: Implementando a Classe de Codificação Posicional\n",
    "\n",
    "Aqui está uma implementação eficiente de codificação posicional no PyTorch. Esta classe `PositionalEncoding` gera codificações baseadas em funções seno e cosseno, que você pode adicionar diretamente às embeddings de entrada.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c96ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encodings matrix with size [max_len, embed_dim]\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        \n",
    "        # Apply sin to even indices in embedding dimension, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as a buffer to avoid tracking in gradients\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: [1, max_len, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154ccde",
   "metadata": {},
   "source": [
    "## Dica de Otimização\n",
    "\n",
    "Se você deseja gerar codificações posicionais dinamicamente para sequências mais longas ou economizar memória, considere criar as codificações posicionais somente quando necessário, em vez de pré-computá-las.\n",
    "\n",
    "Essa abordagem minimiza o uso de memória, especialmente ao trabalhar com sequências muito grandes ou múltiplas sequências simultaneamente.\n",
    "\n",
    "---\n",
    "\n",
    "## Testando a Camada de Atenção\n",
    "\n",
    "Você pode estar se perguntando: “Como posso ter certeza de que minha camada de atenção está funcionando como esperado?” É aqui que os testes unitários entram em cena.\n",
    "\n",
    "Escrever testes para componentes individuais, como scores de atenção, dimensões e fluxo de gradientes, pode economizar muito tempo de depuração, especialmente com mecanismos de atenção personalizados.\n",
    "\n",
    "---\n",
    "\n",
    "## Testes Unitários: Verificando os Outputs de Atenção\n",
    "\n",
    "Os testes envolvem garantir que as dimensões dos outputs de atenção e dos pesos estão corretas e que a camada se comporta de forma consistente em diferentes execuções.\n",
    "\n",
    "Vamos usar o `torch.allclose` do PyTorch para validar se os outputs estão próximos dos valores esperados, dentro de uma pequena tolerância.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAttentionLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.embed_dim = 64\n",
    "        self.num_heads = 8\n",
    "        self.seq_len = 10\n",
    "        self.attention_layer = AttentionLayer(embed_dim=self.embed_dim, num_heads=self.num_heads)\n",
    "        self.input_tensor = torch.randn(1, self.seq_len, self.embed_dim)\n",
    "\n",
    "    def test_attention_output_shape(self):\n",
    "        output, attn_weights = self.attention_layer(self.input_tensor)\n",
    "        self.assertEqual(output.shape, (1, self.seq_len, self.embed_dim))\n",
    "        self.assertEqual(attn_weights.shape, (1, self.num_heads, self.seq_len, self.seq_len))\n",
    "\n",
    "    def test_attention_weights_sum(self):\n",
    "        _, attn_weights = self.attention_layer(self.input_tensor)\n",
    "        self.assertTrue(torch.allclose(attn_weights.sum(dim=-1), torch.tensor(1.0), atol=1e-6))\n",
    "\n",
    "    def test_gradients_exist(self):\n",
    "        output, _ = self.attention_layer(self.input_tensor)\n",
    "        output.sum().backward()\n",
    "        for param in self.attention_layer.parameters():\n",
    "            self.assertIsNotNone(param.grad)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0724c1",
   "metadata": {},
   "source": [
    "## Explicação\n",
    "\n",
    "- **Validação de Forma:** Verifique se o tensor de output e os pesos de atenção correspondem às dimensões esperadas.\n",
    "- **Soma dos Pesos de Atenção:** Confirme que a soma dos pesos de atenção ao longo da última dimensão está próxima de 1 (já que o softmax é usado).\n",
    "- **Verificação de Gradientes:** Certifique-se de que os gradientes existem para cada parâmetro na camada de atenção, confirmando que a retropropagação está funcionando como esperado.\n",
    "\n",
    "Esses testes cobrem os principais aspectos da funcionalidade, proporcionando confiança de que a camada de atenção se comporta corretamente e está pronta para integração.\n",
    "\n",
    "\n",
    "## Exemplo de Uso em uma Arquitetura de Modelo\n",
    "\n",
    "Então, você tem uma camada de atenção personalizada. Agora, vamos vê-la em ação integrando-a em um modelo maior.\n",
    "\n",
    "Aqui está um exemplo de uso dessa camada de atenção dentro de uma arquitetura simples semelhante a uma RNN, que demonstra como a atenção pode ser versátil quando usada junto com outros componentes de deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Código: Integrando a Camada de Atenção em um Modelo\n",
    "\n",
    "Vamos criar um modelo de exemplo que combina uma camada LSTM com atenção.\n",
    "\n",
    "Essa configuração é comum em modelos de NLP, onde a LSTM fornece codificação temporal e a camada de atenção se concentra seletivamente em características críticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_dim, num_heads):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = AttentionLayer(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Example: outputting a single value (e.g., for regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "        attn_out, attn_weights = self.attention(lstm_out)  # Apply attention on LSTM output\n",
    "        final_output = self.fc(attn_out[:, -1, :])  # Example: using the last time step\n",
    "        \n",
    "        return final_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ee7c4",
   "metadata": {},
   "source": [
    "## Explicação\n",
    "\n",
    "- **Camada LSTM:** A LSTM processa dados sequenciais, capturando dependências temporais.  \n",
    "- **Camada de Atenção:** O output da LSTM é passado para a camada de atenção, que refina o foco nas características, potencialmente melhorando a compreensão do modelo sobre as características críticas.  \n",
    "- **Output Final:** Para simplificar, aplicamos uma camada totalmente conectada ao último passo temporal, simulando a configuração para uma tarefa de regressão.  \n",
    "- **Bloco Final de Código:** Exemplo de Forward Pass.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example forward pass\n",
    "model = LSTMWithAttention(input_dim=128, hidden_dim=64, embed_dim=64, num_heads=8)\n",
    "example_input = torch.randn(32, 10, 128)  # Batch of 32, sequence length of 10, input dim of 128\n",
    "output, attn_weights = model(example_input)\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [32, 1]\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected shape: [32, 8, 10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da4334",
   "metadata": {},
   "source": [
    "Neste exemplo, integramos a atenção dentro de um modelo LSTM, mas você pode facilmente aplicar essa técnica a várias arquiteturas, incluindo CNNs ou redes totalmente conectadas.\n",
    "\n",
    "O design modular do mecanismo de atenção o torna compatível com diferentes tipos de modelos, agregando valor onde o foco refinado em características é benéfico.\n",
    "\n",
    "Com essas etapas cobertas, você agora possui um mecanismo de atenção completo e testado, pronto para ser implementado em arquiteturas complexas.\n",
    "\n",
    "Essa configuração, junto com os testes e a integração, oferece ao seu modelo um poder interpretativo avançado e um foco dinâmico em características, que podem melhorar o desempenho em tarefas complexas.\n",
    "\n",
    "---\n",
    "\n",
    "## Considerações de Desempenho e Otimizações\n",
    "\n",
    "Quando se trata de escalar camadas de atenção, cada pequena otimização faz diferença.\n",
    "\n",
    "A realidade é esta: tensores grandes e matrizes de alta dimensionalidade podem consumir memória rapidamente, especialmente em modelos de atenção profundos.\n",
    "\n",
    "Ajustando a precisão e executando benchmarks de desempenho, você pode manter o uso de memória e os tempos de computação sob controle.\n",
    "\n",
    "---\n",
    "\n",
    "### Lidando com Tensores Grandes usando Treinamento de Precisão Mista\n",
    "\n",
    "Uma maneira de reduzir o consumo de memória sem sacrificar a qualidade do modelo é usar o treinamento de precisão mista.\n",
    "\n",
    "Esse método utiliza o `torch.cuda.amp` (precisão mista automática), que converte operações para meia precisão (`float16`) sempre que possível, enquanto mantém cálculos críticos em precisão total (`float32`).\n",
    "\n",
    "Essa estratégia pode aumentar o desempenho, permitindo que os modelos treinem mais rápido com menor uso de memória.\n",
    "\n",
    "Aqui está uma configuração prática para implementar precisão mista com o `torch.cuda.amp` do PyTorch:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaff65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Example model, optimizer, and scaler setup\n",
    "model = AttentionLayer(embed_dim=64, num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()  # For automatic gradient scaling\n",
    "\n",
    "# Mixed-precision training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():  # Enable mixed precision\n",
    "            output, _ = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "        \n",
    "        # Backpropagation with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee2eee",
   "metadata": {},
   "source": [
    "## Explicação\n",
    "\n",
    "- **autocast():** Executa todas as operações dentro deste bloco em precisão mista, reduzindo o uso de memória enquanto mantém a precisão.  \n",
    "- **GradScaler:** Ajusta os gradientes para evitar underflow em `float16`, especialmente útil ao treinar com dados sensíveis ou de alta resolução.  \n",
    "\n",
    "Essa abordagem não só economiza memória, mas também acelera o treinamento, tornando-a ideal para modelos em larga escala baseados em atenção.\n",
    "\n",
    "---\n",
    "\n",
    "## Dicas de Benchmarking para Computações de Atenção\n",
    "\n",
    "Para entender o desempenho da sua camada de atenção, você pode usar o `torch.utils.benchmark`, que permite medir o tempo das suas computações e identificar possíveis gargalos.  \n",
    "\n",
    "O benchmarking é especialmente útil ao testar diferentes configurações de modelo ou realizar verificações de desempenho em várias configurações de hardware.\n",
    "\n",
    "---\n",
    "\n",
    "## Código: Benchmarking da Computação de Atenção\n",
    "\n",
    "Vamos configurar um script rápido de benchmarking para analisar o tempo gasto pela nossa camada de atenção.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80758214",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport torch.utils.benchmark as benchmark\n",
    "\n",
    "# Sample data for benchmarking\n",
    "input_tensor = torch.randn(32, 10, 64).to(device)\n",
    "\n",
    "# Create benchmark timer\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"model(input_tensor)\",\n",
    "    globals={\"model\": model, \"input_tensor\": input_tensor},\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "time_taken = timer.timeit(100)  # Run the forward pass 100 times\n",
    "print(f\"Average time per forward pass: {time_taken.mean * 1e3:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a35f4",
   "metadata": {},
   "source": [
    "## Explicação\n",
    "\n",
    "- **benchmark.Timer:** Mede o tempo médio de computação para a instrução especificada, neste caso, executando o forward pass na camada de atenção.  \n",
    "- **Interpretação dos Resultados:** Use o tempo médio por forward pass para avaliar a eficiência do desempenho do seu modelo e compará-lo em diferentes configurações (por exemplo, CPU vs. GPU, precisão mista vs. total).  \n",
    "\n",
    "Seguindo essas dicas de desempenho, você garante que sua camada de atenção não seja apenas funcional, mas também otimizada para aplicações em larga escala e em tempo real.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão e Aperfeiçoamentos Futuros\n",
    "\n",
    "Agora que você dominou os fundamentos de construção, otimização e implementação de mecanismos de atenção no PyTorch, para onde ir a partir daqui?\n",
    "\n",
    "Aqui vai uma ideia: considere experimentar autoatenção dentro de camadas convolucionais.\n",
    "\n",
    "As camadas de atenção não se limitam a Transformers; elas estão sendo cada vez mais usadas em visão computacional para segmentação de imagens e aprimoramento, onde podem adicionar consciência espacial às convoluções.\n",
    "\n",
    "Além disso, a atenção baseada em grafos pode desbloquear novos potenciais em aplicações que envolvem dados relacionais, como redes sociais ou estruturas moleculares.\n",
    "\n",
    "Com isso, você será capaz de modelar relações complexas ao estender camadas de atenção para estruturas de dados em grafos, o que pode levar a previsões mais refinadas.\n",
    "\n",
    "---\n",
    "\n",
    "## Leituras Sugeridas e Extensões Avançadas\n",
    "\n",
    "Para se aprofundar em mecanismos de atenção, considere explorar:\n",
    "\n",
    "- **Bibliotecas de Transformers eficientes**, como os `transformers` da Hugging Face e o `torchtext` do PyTorch, para componentes otimizados de Transformers.  \n",
    "- **Atenção multimodal:** Combinar atenção em diferentes tipos de dados, como texto e imagens, pode aumentar significativamente o desempenho em modelos multimodais.  \n",
    "\n",
    "Com esses recursos e ideias, você estará bem equipado para construir modelos ainda mais sofisticados e versáteis, ampliando os limites do que os mecanismos de atenção podem alcançar no aprendizado de máquina moderno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a325ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1283a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
